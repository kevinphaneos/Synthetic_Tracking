{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Uploading data to CoLab:\n",
        "I named my dataset folder yoloset.zip\n",
        "\n",
        "Data was generated by SatSim, formatted by annotator.py on Kevin's GitHub"
      ],
      "metadata": {
        "id": "s_23v_K4vE0-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7nutXyMuvIw"
      },
      "outputs": [],
      "source": [
        "! unzip yoloset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Prebuilt Model from ultralytics\n"
      ],
      "metadata": {
        "id": "IDMJhRB4vkbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "LlLYH7Ig0QvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define task specific head. Our task is object detection. Code from modified from documentation. Remember to drag in python file dependencies before running. This is for modifying the architecture; ignore this cell if you are going to train on default parameters and architectures. refer to docs here: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/nn/modules/head.py"
      ],
      "metadata": {
        "id": "WJtVHwHywHqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.init import constant_, xavier_uniform_\n",
        "\n",
        "from ultralytics.utils.tal import TORCH_1_10, dist2bbox, dist2rbox, make_anchors\n",
        "from .block import DFL, Proto, ContrastiveHead, BNContrastiveHead\n",
        "from .conv import Conv\n",
        "from .transformer import MLP, DeformableTransformerDecoder, DeformableTransformerDecoderLayer\n",
        "from .utils import bias_init_with_prob, linear_init\n",
        "\n",
        "class Detect(nn.Module):\n",
        "    \"\"\"YOLOv8 Detect head for detection models.\"\"\"\n",
        "\n",
        "    dynamic = False  # force grid reconstruction\n",
        "    export = False  # export mode\n",
        "    shape = None\n",
        "    anchors = torch.empty(0)  # init\n",
        "    strides = torch.empty(0)  # init\n",
        "\n",
        "    def __init__(self, nc=80, ch=()):\n",
        "        \"\"\"Initializes the YOLOv8 detection layer with specified number of classes and channels.\"\"\"\n",
        "        super().__init__()\n",
        "        self.nc = nc  # number of classes\n",
        "        self.nl = len(ch)  # number of detection layers\n",
        "        self.reg_max = 16  # DFL channels (ch[0] // 16 to scale 4/8/12/16/20 for n/s/m/l/x)\n",
        "        self.no = nc + self.reg_max * 4  # number of outputs per anchor\n",
        "        self.stride = torch.zeros(self.nl)  # strides computed during build\n",
        "        c2, c3 = max((16, ch[0] // 4, self.reg_max * 4)), max(ch[0], min(self.nc, 100))  # channels\n",
        "        self.cv2 = nn.ModuleList(\n",
        "            nn.Sequential(Conv(x, c2, 3), Conv(c2, c2, 3), nn.Conv2d(c2, 4 * self.reg_max, 1)) for x in ch\n",
        "        )\n",
        "        self.cv3 = nn.ModuleList(nn.Sequential(Conv(x, c3, 3), Conv(c3, c3, 3), nn.Conv2d(c3, self.nc, 1)) for x in ch)\n",
        "        self.dfl = DFL(self.reg_max) if self.reg_max > 1 else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Concatenates and returns predicted bounding boxes and class probabilities.\"\"\"\n",
        "        for i in range(self.nl):\n",
        "            x[i] = torch.cat((self.cv2[i](x[i]), self.cv3[i](x[i])), 1)\n",
        "        if self.training:  # Training path\n",
        "            return x\n",
        "\n",
        "        # Inference path\n",
        "        shape = x[0].shape  # BCHW\n",
        "        x_cat = torch.cat([xi.view(shape[0], self.no, -1) for xi in x], 2)\n",
        "        if self.dynamic or self.shape != shape:\n",
        "            self.anchors, self.strides = (x.transpose(0, 1) for x in make_anchors(x, self.stride, 0.5))\n",
        "            self.shape = shape\n",
        "\n",
        "        if self.export and self.format in (\"saved_model\", \"pb\", \"tflite\", \"edgetpu\", \"tfjs\"):  # avoid TF FlexSplitV ops\n",
        "            box = x_cat[:, : self.reg_max * 4]\n",
        "            cls = x_cat[:, self.reg_max * 4 :]\n",
        "        else:\n",
        "            box, cls = x_cat.split((self.reg_max * 4, self.nc), 1)\n",
        "\n",
        "        if self.export and self.format in (\"tflite\", \"edgetpu\"):\n",
        "            grid_h = shape[2]\n",
        "            grid_w = shape[3]\n",
        "            grid_size = torch.tensor([grid_w, grid_h, grid_w, grid_h], device=box.device).reshape(1, 4, 1)\n",
        "            norm = self.strides / (self.stride[0] * grid_size)\n",
        "            dbox = self.decode_bboxes(self.dfl(box) * norm, self.anchors.unsqueeze(0) * norm[:, :2])\n",
        "        else:\n",
        "            dbox = self.decode_bboxes(self.dfl(box), self.anchors.unsqueeze(0)) * self.strides\n",
        "\n",
        "        y = torch.cat((dbox, cls.sigmoid()), 1)\n",
        "        return y if self.export else (y, x)\n",
        "\n",
        "    def bias_init(self):\n",
        "        \"\"\"Initialize Detect() biases, WARNING: requires stride availability.\"\"\"\n",
        "        m = self  # self.model[-1]  # Detect() module\n",
        "        # cf = torch.bincount(torch.tensor(np.concatenate(dataset.labels, 0)[:, 0]).long(), minlength=nc) + 1\n",
        "        # ncf = math.log(0.6 / (m.nc - 0.999999)) if cf is None else torch.log(cf / cf.sum())  # nominal class frequency\n",
        "        for a, b, s in zip(m.cv2, m.cv3, m.stride):  # from\n",
        "            a[-1].bias.data[:] = 1.0  # box\n",
        "            b[-1].bias.data[: m.nc] = math.log(5 / m.nc / (640 / s) ** 2)  # cls (.01 objects, 80 classes, 640 img)\n",
        "\n",
        "    def decode_bboxes(self, bboxes, anchors):\n",
        "        \"\"\"Decode bounding boxes.\"\"\"\n",
        "        return dist2bbox(bboxes, anchors, xywh=True, dim=1)"
      ],
      "metadata": {
        "id": "iejMWtZ0xnKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run model training. Set the data variable to the path of wherever the data.yaml file is. Note: the data.yaml file may have to be modified, specifically the paths of the training and valid set as google colab directories are different."
      ],
      "metadata": {
        "id": "m6vuPDV0wv3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo task=detect mode=train model=yolov8m.pt data=/content/data.yaml epochs=20 imgsz=512"
      ],
      "metadata": {
        "id": "vUxxWvdOwmlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the results:"
      ],
      "metadata": {
        "id": "ahWJzJ1Cymnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r downloading.zip .\n",
        "!du sh downloading.zip\n",
        "from google.colab import files\n",
        "files.download('downloading.zip')"
      ],
      "metadata": {
        "id": "759lM33mxEc0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}